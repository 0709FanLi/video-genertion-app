# 图生视频（Image-to-Video）功能详解

## 📖 什么是图生视频？

图生视频（Image-to-Video，简称I2V）是一种AI技术，它使用一张静态图片作为**参考/基础**，结合文本提示词，生成动态视频。

## 🎬 通义万相 wanx-i2v-v1 的工作原理

### 核心机制

通义万相的图生视频API (`wanx-i2v-v1`) 接受两个关键输入：

```python
{
    "img_url": "https://example.com/image.jpg",  # 参考图片
    "prompt": "海浪轻轻拍打，猫的尾巴在摇摆"     # 动态描述
}
```

### 图片的作用

**图片主要有三个作用：**

#### 1. ✅ 作为视频的首帧（第一帧）
- 生成的视频会**以这张图片作为起始画面**
- 视频的第一秒会保持图片的主要构图
- 确保视频内容与图片保持一致性

#### 2. ✅ 作为视觉风格参考
- AI会学习图片的：
  - 色彩风格（色调、饱和度）
  - 光线条件（光照方向、强度）
  - 艺术风格（写实、卡通、油画等）
  - 构图布局（主体位置、背景元素）

#### 3. ✅ 作为内容约束
- 视频中的主体必须是图片中的主体
- 不会凭空出现图片中没有的元素
- 保持场景的连贯性

## 🎥 实际效果示例

### 示例1：海滩上的猫

**输入图片**：
- 一只橘色的猫坐在沙滩上
- 背景是蓝色的海洋
- 晴朗的天气，阳光明媚

**提示词**：
```
海浪轻轻拍打沙滩，猫的尾巴在微风中摇摆，
镜头缓慢推进，电影级画质
```

**生成视频效果**：
- **第1帧**：完全是输入的图片
- **后续帧**：
  - 猫保持坐姿，但尾巴开始摆动
  - 远处的海浪有轻微波动
  - 猫的毛发随风轻动
  - 镜头逐渐推近
  - **重点**：猫的位置、颜色、周围环境都与原图一致

### 示例2：森林中的宇航员

**输入图片**：
- 一个宇航员站在森林中
- 周围是茂密的树木
- 夜晚场景，有月光

**提示词**：
```
宇航员抬头仰望星空，树叶在风中轻轻摇曳，
月光透过树叶洒下斑驳的光影
```

**生成视频效果**：
- **第1帧**：就是输入的图片
- **后续帧**：
  - 宇航员的头部缓缓抬起
  - 树叶轻微晃动
  - 光影有细微变化
  - **重点**：宇航员的外观、森林环境完全保留

## 🔍 图生视频 vs 文生视频

| 特性 | 图生视频 (I2V) | 文生视频 (T2V) |
|------|---------------|---------------|
| **输入** | 图片 + 提示词 | 仅提示词 |
| **首帧** | 固定（就是输入图片） | AI自由生成 |
| **可控性** | ⭐⭐⭐⭐⭐ 非常高 | ⭐⭐⭐ 中等 |
| **一致性** | ⭐⭐⭐⭐⭐ 极高 | ⭐⭐⭐ 中等 |
| **创意自由** | ⭐⭐⭐ 受图片约束 | ⭐⭐⭐⭐⭐ 完全自由 |
| **适用场景** | 需要精确控制内容时 | 创意探索 |

## 💡 当前项目的工作流程

```
步骤1: 用户输入想法
   ↓
步骤2: AI生成3个图片提示词
   ↓
步骤3: 选择1个提示词
   ↓
步骤4: 生成4张候选图片 ← 文生图 (T2I)
   ↓
步骤5: 用户选择1张图片 ← 这张图片将作为视频首帧！
   ↓
步骤6: 用户输入视频动态描述
   ↓
步骤7: 生成视频 ← 图生视频 (I2V)
   ↓
结果: 视频的第一帧就是步骤5选择的图片
```

## 🎯 图片在视频中的具体表现

### 测试案例：猫在沙滩

**原始图片内容**：
- 猫的品种：橘猫
- 猫的姿势：坐着
- 猫的位置：沙滩中央
- 背景：蓝色海洋 + 白云
- 光线：明亮的日光

**提示词1（小动作）**：
```
猫的尾巴轻轻摇摆
```
**视频效果**：
- ✅ 猫保持坐姿
- ✅ 仅尾巴有动作
- ✅ 背景静态
- ✅ 画面几乎与原图一致

**提示词2（环境变化）**：
```
海浪拍打沙滩，天空云朵飘动
```
**视频效果**：
- ✅ 猫保持静态
- ✅ 远处海浪波动
- ✅ 云朵缓慢移动
- ✅ 猫的外观完全保留

**提示词3（镜头运动）**：
```
镜头缓慢推进，聚焦猫的眼睛
```
**视频效果**：
- ✅ 猫保持静态
- ✅ 画面逐渐放大
- ✅ 焦点变化
- ✅ 内容完全一致

## 🚫 图生视频的限制

### 不能做的事情：

1. **不能改变主体**
   - ❌ 图片是猫 → 不能变成狗
   - ❌ 一只猫 → 不能变成两只猫

2. **不能大幅改变场景**
   - ❌ 沙滩 → 不能变成森林
   - ❌ 白天 → 不能变成夜晚

3. **不能添加新元素**
   - ❌ 空白的天空 → 不能添加鸟
   - ❌ 图片里没有的物体 → 提示词也不会出现

4. **不能大幅度动作**
   - ❌ 坐着的猫 → 跳跃（动作太大）
   - ❌ 静止的人 → 奔跑（变化太剧烈）

### 能做的事情：

1. **微小动作**
   - ✅ 眨眼、摇尾巴、转头
   - ✅ 轻微的肢体移动

2. **环境变化**
   - ✅ 海浪波动、云朵移动
   - ✅ 树叶摇摆、光影变化

3. **镜头运动**
   - ✅ 推、拉、摇、移
   - ✅ 焦点变化

4. **细微效果**
   - ✅ 毛发/衣物飘动
   - ✅ 光线变化
   - ✅ 大气效果（雾气、烟雾）

## 🎨 最佳实践建议

### 选图技巧

选择一张**高质量**的图片：
- ✅ 主体清晰、居中
- ✅ 构图稳定
- ✅ 光线自然
- ✅ 背景简洁
- ✅ 分辨率高（1024x1024以上）

### 提示词编写技巧

**好的提示词**：
```
海浪轻轻拍打沙滩，泛起白色泡沫，
猫的尾巴在微风中优雅摆动，
耳朵微微转动，目光望向远方，
镜头缓慢推进，电影级画质，高清
```

**特点**：
- ✅ 描述具体的动作
- ✅ 动作幅度小而自然
- ✅ 包含环境变化
- ✅ 指定镜头运动
- ✅ 添加画质要求

**不好的提示词**：
```
猫在沙滩上奔跑玩耍
```

**问题**：
- ❌ 动作太大（从坐姿到奔跑）
- ❌ 与原图冲突
- ❌ 描述不够具体

## 🔬 技术原理（简化说明）

```
输入图片 ────────┐
                  ↓
             [AI模型]
                  ↓
          提取特征：
          - 主体外观
          - 场景布局  ←─── 作为约束条件
          - 风格特征
                  ↓
输入提示词 ───→ [AI模型]
                  ↓
          生成动作：
          - 微小变化
          - 保持一致性
                  ↓
          输出视频帧序列
```

## 📊 视频生成参数（当前项目）

```python
{
    "model": "wanx-i2v-v1",
    "input": {
        "img_url": "https://...",      # 参考图片（成为首帧）
        "prompt": "描述动态..."         # 动态描述
    }
}
```

**当前不支持的参数**（可能在未来版本支持）：
- ❌ `duration`: 视频时长（目前固定）
- ❌ `motion_strength`: 动作强度
- ❌ `camera_motion`: 镜头运动强度

## 🎯 总结

### 关键要点：

1. **图片就是视频的首帧**
   - 视频从这张图片开始
   - 第0秒画面 = 输入图片

2. **图片定义了内容边界**
   - 视频内容不会偏离图片太远
   - 主体、场景、风格都被锁定

3. **提示词描述动态变化**
   - 专注于描述"怎么动"
   - 而不是"长什么样"（已被图片定义）

4. **适合微动画**
   - 最适合细微、自然的动作
   - 不适合大幅度的场景切换

### 如果您想要：

**方案A：完全控制内容** → 使用图生视频（当前方案）✅  
**方案B：创意探索** → 使用文生视频（纯提示词）  
**方案C：先改图再生成视频** → 需要图生图功能（待调研）

---

**文档版本**: 1.0  
**最后更新**: 2025-10-24  
**适用项目**: AI视频创作工作流系统

